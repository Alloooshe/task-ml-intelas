{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from custom_dataset import LedgerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'testing.xlsx'\n",
    "save_json_name ='test.json'\n",
    "model_name = \"output_base_3/checkpoint-10000/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(file_name, sheet_name=None, index_col=None)\n",
    "df = pd.concat(df.values(),ignore_index=True)\n",
    "target_column = \"target\"\n",
    "source_column = \"source\"\n",
    "\n",
    "df = df.rename(columns={\"Intelas Ledger Name\": target_column, \"Source Ledger Name\": source_column})\n",
    "df_to_save = df.apply(lambda x: {'id':str(x.name), 'translation': {'source' : x[source_column], 'target' : x[target_column]} }, axis=1)\n",
    "with open(save_json_name, 'w') as fout:\n",
    "    json.dump(list(df_to_save.to_dict().values()) , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "translator = pipeline(\"translation_source_to_target\", model=model_name,max_length=30)\n",
    "test_dataset = LedgerDataset(save_json_name,tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"translate source to target: \"\n",
    "res = []\n",
    "for item in test_dataset : \n",
    "    text = prefix + item[\"translation\"][\"source\"]\n",
    "    target_text = item[\"translation\"][\"target\"]\n",
    "    pred = translator(text)[0]['translation_text']\n",
    "    res.append({\n",
    "        \"text\":text, \n",
    "        \"target\" : target_text,\n",
    "        \"pred\" : pred, \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets =[x[\"target\"] for x in res]\n",
    "predictions = [x[\"pred\"] for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target : Restricted Cash - General\n",
      " prediction : Restricted Cash - Rehab Escrow\n",
      "target : Intercompany Liability\n",
      " prediction : Intercompany Asset\n",
      "target : Transfer / Sublet  Fee\n",
      " prediction : Transfer / Sublet Fee\n",
      "target : Plumbing - Repairs\n",
      " prediction : Fixed Assets - General\n",
      "target : Common Area - Exterior\n",
      " prediction : Fixed Assets - General\n",
      "target : Common Area - General\n",
      " prediction : Fixed Assets - General\n",
      "accuracy : \n",
      "0.9707317073170731\n",
      "0.02926829268292683\n"
     ]
    }
   ],
   "source": [
    "sum =0 \n",
    "failed= 0 \n",
    "for i in range(len(targets)): \n",
    "    if targets[i] == predictions[i]:\n",
    "        sum+=1\n",
    "    else : \n",
    "        failed +=1 \n",
    "        print(f\"target : {targets[i]}\\n prediction : {predictions[i]}\")\n",
    "print(\"accuracy : \")\n",
    "print(sum/len(targets))\n",
    "print(failed/len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali-mhd/task-ml-intelas/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ali-mhd/task-ml-intelas/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.963821138211382, 0.9707317073170731, 0.9641347270615563, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(np.array(targets), np.array(predictions), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4884395e646e0a375baeb8d9c0cde3d28b17c210e29da863a0ad1228d6d7b91f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
